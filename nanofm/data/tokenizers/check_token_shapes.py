import numpy as np
from pathlib import Path
import argparse
from collections import Counter

def analyze_vocab_size(token_dir: Path, num_files: int = 50):
    """
    Analyse la taille du vocabulaire utilis√© dans les tokens audio.
    
    Args:
        token_dir: R√©pertoire contenant les fichiers .npy
        num_files: Nombre de fichiers √† analyser
    """
    print(f"üîç Analyse du vocabulaire des tokens audio")
    print(f"üìÅ R√©pertoire: {token_dir}")
    print(f"üìä Analysing {num_files} fichiers...\n")
    
    # Collecter tous les tokens
    all_tokens = []
    shapes = []
    
    token_files = list(token_dir.glob("*.npy"))[:num_files]
    
    if not token_files:
        print(f"‚ùå Aucun fichier .npy trouv√© dans {token_dir}")
        return
    
    for i, token_file in enumerate(token_files):
        try:
            tokens = np.load(token_file)
            all_tokens.append(tokens)
            shapes.append(tokens.shape)
            
            if i % 10 == 0:
                print(f"  Traitement: {i+1}/{len(token_files)} fichiers...")
                
        except Exception as e:
            print(f"‚ùå Erreur avec {token_file.name}: {e}")
    
    if not all_tokens:
        print("‚ùå Aucun token valide trouv√©!")
        return
    
    # Analyser par codebook (colonne)
    print(f"\n{'='*70}")
    print("üìä ANALYSE PAR CODEBOOK")
    print(f"{'='*70}")
    
    # D√©terminer le nombre de codebooks
    num_codebooks = all_tokens[0].shape[1]
    print(f"üî¢ Nombre de codebooks: {num_codebooks}")
    
    vocab_sizes_per_codebook = []
    
    for codebook_idx in range(num_codebooks):
        # Collecter tous les tokens pour ce codebook
        codebook_tokens = []
        for tokens in all_tokens:
            codebook_tokens.extend(tokens[:, codebook_idx].flatten())
        
        # Calculer les statistiques
        unique_tokens = np.unique(codebook_tokens)
        vocab_size = len(unique_tokens)
        min_token = min(codebook_tokens)
        max_token = max(codebook_tokens)
        
        vocab_sizes_per_codebook.append(vocab_size)
        
        print(f"  Codebook {codebook_idx:2d}: "
              f"Vocab={vocab_size:4d}, Range=[{min_token:4d}, {max_token:4d}]")
        
        # Montrer la distribution pour les premiers codebooks
        if codebook_idx < 3:
            counter = Counter(codebook_tokens)
            most_common = counter.most_common(5)
            print(f"    Top 5 tokens: {most_common}")
    
    # R√©sum√© global
    print(f"\n{'='*70}")
    print("üìà R√âSUM√â GLOBAL")
    print(f"{'='*70}")
    
    print(f"üî¢ Vocab size par codebook:")
    print(f"  Min: {min(vocab_sizes_per_codebook)}")
    print(f"  Max: {max(vocab_sizes_per_codebook)}")
    print(f"  Moyenne: {np.mean(vocab_sizes_per_codebook):.1f}")
    
    # V√©rifier la coh√©rence
    all_vocab_same = len(set(vocab_sizes_per_codebook)) == 1
    print(f"üéØ Vocabulaire uniforme: {'‚úÖ Oui' if all_vocab_same else '‚ùå Non'}")
    
    if all_vocab_same:
        vocab_size = vocab_sizes_per_codebook[0]
        print(f"üéâ Taille du vocabulaire: {vocab_size}")
        
        # V√©rifier si c'est une puissance de 2
        import math
        if vocab_size > 0 and (vocab_size & (vocab_size - 1)) == 0:
            bits = int(math.log2(vocab_size))
            print(f"üí° C'est 2^{bits} (puissance de 2)")
    
    # Analyse additionnelle
    print(f"\nüìã INFORMATIONS ADDITIONNELLES:")
    
    # Calculer la densit√© d'utilisation
    theoretical_max = 2048  # Vocab size th√©orique de Mimi
    if all_vocab_same and vocab_sizes_per_codebook[0] <= theoretical_max:
        usage_density = vocab_sizes_per_codebook[0] / theoretical_max * 100
        print(f"üìä Densit√© d'utilisation du vocab th√©orique: {usage_density:.1f}%")
    
    # Analyser les shapes
    unique_shapes = list(set(shapes))
    print(f"üî≥ Shapes trouv√©es: {unique_shapes}")
    
    # Estimer l'efficacit√© de compression
    total_tokens = sum(tokens.size for tokens in all_tokens)
    total_files = len(all_tokens)
    avg_tokens_per_file = total_tokens / total_files
    print(f"üìè Tokens moyens par fichier: {avg_tokens_per_file:.0f}")

def check_specific_vocab_properties(token_path: Path):
    """
    V√©rifications sp√©cifiques pour Mimi.
    """
    print(f"üî¨ V√©rification des propri√©t√©s Mimi pour: {token_path.name}")
    
    tokens = np.load(token_path)
    
    # V√©rifications Mimi
    expected_codebooks = 32
    expected_vocab_size = 2048
    expected_range = (0, 2047)
    
    print(f"üìê Shape: {tokens.shape}")
    print(f"üîç V√©rifications Mimi:")
    
    # Nombre de codebooks
    actual_codebooks = tokens.shape[1] if tokens.ndim == 2 else 1
    print(f"  Codebooks: {actual_codebooks} {'‚úÖ' if actual_codebooks == expected_codebooks else '‚ùå'} "
          f"(attendu: {expected_codebooks})")
    
    # Range des valeurs
    min_val, max_val = tokens.min(), tokens.max()
    range_ok = min_val >= expected_range[0] and max_val <= expected_range[1]
    print(f"  Range: [{min_val}, {max_val}] {'‚úÖ' if range_ok else '‚ùå'} "
          f"(attendu: {expected_range})")
    
    # Vocab size utilis√©
    unique_tokens = len(np.unique(tokens))
    print(f"  Tokens uniques: {unique_tokens} {'‚úÖ' if unique_tokens <= expected_vocab_size else '‚ùå'} "
          f"(max: {expected_vocab_size})")

def main():
    parser = argparse.ArgumentParser("Analyser la vocab size des tokens audio")
    parser.add_argument("token_path", type=Path,
                       help="Chemin vers le r√©pertoire ou fichier token")
    parser.add_argument("--num_files", type=int, default=50,
                       help="Nombre de fichiers √† analyser")
    parser.add_argument("--quick_check", action="store_true",
                       help="V√©rification rapide d'un seul fichier")
    
    args = parser.parse_args()
    
    if args.token_path.is_file():
        # Analyse d'un seul fichier
        check_specific_vocab_properties(args.token_path)
    elif args.token_path.is_dir():
        # Analyse compl√®te du r√©pertoire
        analyze_vocab_size(args.token_path, args.num_files)
        
        # V√©rification rapide d'un √©chantillon
        sample_file = next(args.token_path.glob("*.npy"), None)
        if sample_file:
            print(f"\n{'='*70}")
            check_specific_vocab_properties(sample_file)
    else:
        print(f"‚ùå Chemin non valide: {args.token_path}")

if __name__ == "__main__":
    main()