{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin: 0 auto 30px; height: 60px; border: 2px solid gray; border-radius: 6px;\">\n",
    "  <div style=\"float: left;\"><img src=\"img/epfl.png\" /></div>\n",
    "  <div style=\"float: right; margin: 20px 30px 0; font-size: 10pt; font-weight: bold;\"><a href=\"https://moodle.epfl.ch/course/view.php?id=18345\">COM304 - Communication Project</a></div>\n",
    "</div>\n",
    "<div style=\"clear: both; font-size: 30pt; font-weight: bold; color: #483D8B;\">\n",
    "    Demo Notebook SAGA group, nano4M with Audio/Video\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Welcome to our COM304 Communication Project demo notebook! In this session, we showcase the result that we get from our different extansions. Developed by the SAGA group. Throughout this notebook, we will:\n",
    "\n",
    "- Setup the necessary import + define some helper function to help us visualise results.\n",
    "- Visualise some sample of our data, pre/post tokenization.\n",
    "- Go through the different results that we obtain trough different training on different modalities.\n",
    "- Visualise and compare the result that we get when fine tuning an audio tokenizer on our data.\n",
    "\n",
    "Let’s start by importing the necessary libraries below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/godey/SAGA_COM-304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/com-304/SAGA/.envs/sagaperso/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/work/com-304/SAGA/.envs/sagaperso/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "### # Switch path to root of project\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "current_folder = globals()['_dh'][0]\n",
    "# project_root = os.path.abspath(os.path.join(current_folder, '..', '..'))\n",
    "# os.chdir(project_root)\n",
    "\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "import io\n",
    "from IPython.display import display,Audio\n",
    "from IPython.display import Image as IPyImage\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from nanofm.data.utils import save_video\n",
    "\n",
    "from nanofm.data.utils import save_video\n",
    "from nanofm.utils.checkpoint import load_safetensors, load_state_dict\n",
    "from nanofm.data.multimodal.masking import SimpleMultimodalMasking\n",
    "from nanofm.data.multimodal.adapted_multimodal_dataset import AdaptedMultimodalDataset\n",
    "\n",
    "# Tokenizer imports\n",
    "from nanofm.data.tokenizers.image_tokenizer import ImageTokenizer\n",
    "from nanofm.data.tokenizers.audio_tokenizer import AudioTokenizer\n",
    "from nanofm.data.tokenizers.video_tokenizer import VideoTokenizer\n",
    "from nanofm.data.tokenizers.label_map import Maplabel\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# The flag below controls whether to allow TF32 on matmul. This flag defaults to False in PyTorch 1.12 and later.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_grad_enabled(False)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pretrained Models\n",
    "\n",
    "Now that all dependencies are in place, we’ll load our pretrained checkpoints and organize them by training run and modality. To do this, we define a Python dictionary where each key is the run identifier and each value contains:\n",
    "\n",
    "- The path of the model instance  \n",
    "- The corresponding modality it was trained on  \n",
    "\n",
    "```python\n",
    "# Example structure — update with your actual run names & checkpoint paths\n",
    "pretrained_models = {\n",
    "    \"run_audio_tokenizer\": {\n",
    "        \"model\": \"path/to/checkpoint/safetensors\",\n",
    "        \"modality\": \"['modality1', 'modality2', ..., 'modalityN']\"\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = {\n",
    "    \"CKPT_FIRST_TRAIN\": {\n",
    "        \"path\": \"/work/com-304/SAGA/outputs/nano4M/multiclevr_d6-6w512/checkpoint-final.safetensors\",\n",
    "        \"tags\": [\"tok_rgb@256\", \"tok_depth@256\", \"tok_audio@24_000\", \"tok_video@256\"],\n",
    "    },\n",
    "    \"CKPT_FIRST_SMALL_SUBSET\": {\n",
    "        \"path\": \"/work/com-304/SAGA/outputs/nano4M/SAGAnano4M_smallest/checkpoint-final.safetensors\",\n",
    "        \"tags\": [\"tok_rgb@256\", \"tok_depth@256\"], #TO COMPLETE\n",
    "    },\n",
    "    \"CKPT_RGB&CAPT\": {\n",
    "        \"path\": \"/work/com-304/SAGA/outputs/rgb_capt/checkpoint-final.safetensors\",\n",
    "        \"tags\": [\"tok_rgb@256\", \"tok_label\"],\n",
    "    },\n",
    "    \"CKPT_DEPTH&RGB\": {\n",
    "        \"path\": \"/work/com-304/SAGA/outputs/nano4M/SAGAnano4M_depth/checkpoint-final.safetensors\",\n",
    "        \"tags\": [\"tok_rgb@256\", \"tok_depth@256\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "DATA_ROOT = '/work/com-304/SAGA/tokens_16_05/'\n",
    "IMAGE_MODEL_NAME = \"Cosmos-0.1-Tokenizer-DI16x16\"\n",
    "VIDEO_MODEL_NAME = \"Cosmos-0.1-Tokenizer-DV8x8x8\"\n",
    "PATH_LABEL_DICT = \"/home/godey/SAGA_COM-304/dataset_module/data/processed_data/label_counts.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Tokenizers, Model & DataLoader\n",
    "\n",
    "Once our checkpoints are organized, the next step is to load the associated tokenizers, instantiate the model, and wrap our tokenized dataset in a `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizers\n",
    "\n",
    "image_tokenizer = ImageTokenizer(\n",
    "            model_name=IMAGE_MODEL_NAME,\n",
    "            device=torch.device(device)\n",
    "        )\n",
    "audio_tokenizer = AudioTokenizer(device=torch.device(\"cpu\"))\n",
    "video_tokenizer = VideoTokenizer(\n",
    "            model_name=VIDEO_MODEL_NAME,\n",
    "            device=torch.device(device)\n",
    "        )\n",
    "label_tokenizer = Maplabel(PATH_LABEL_DICT)\n",
    "\n",
    "all_modalities = [\n",
    "    'tok_audio@24_000',\n",
    "    'tok_depth@256',\n",
    "    'tok_rgb@256',\n",
    "    'tok_video@256'\n",
    "    'tok_label'\n",
    "]\n",
    "\n",
    "img_dataset2 = AdaptedMultimodalDataset(root_dir = \"/work/com-304/SAGA/tokens_16_05/\",\n",
    "            split = \"eval\" ,\n",
    "            modalities= all_modalities ,\n",
    "            sample_from_k_augmentations = 1)\n",
    "\n",
    "\n",
    "#Load model\n",
    "selected_ckpt, selected_modalities = checkpoints['CKPT_FIRST_TRAIN']\n",
    "\n",
    "ckpt, config = load_safetensors(selected_ckpt)\n",
    "model = instantiate(config)\n",
    "\n",
    "load_state_dict(model, ckpt, ignore_missing='dec_context_proj')\n",
    "model = model.to(device)\n",
    "e = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Now that our data pipelines and models are in place, it’s useful to bundle common routines into helper functions for visualization and data processing. Below are the core utilities we’ll use throughout the notebook:\n",
    "\n",
    "- `show_im_from_tensor(tensor_or_array)`: Convert a (C,H,W) tensor or array into a PIL image.  \n",
    "- `get_gif_bytes_from_tensor(frames: torch.Tensor, fps: int = 3) → bytes`: Turn a (C,T,H,W) tensor into GIF byte data.  \n",
    "- `construct_input_from_sample(dataset, idx, input_modality)`: Build token, position & modality tensors for a dataset sample.  \n",
    "- `token_ids_to_image(token_ids, image_tokenizer, to_pil=False)`: Decode image token IDs back into a tensor or PIL image.  \n",
    "- `tokens_ids_to_audio(token_ids, audio_tokenizer)`: Decode audio token IDs, play the waveform, and return the raw signal.  \n",
    "- `tokens_ids_to_gif(token_ids, video_tokenizer)`: Decode video token IDs and display the resulting frames as a GIF.  \n",
    "- `tokens_ids_to_label(tokens_ids, label_tokenizer)`: Decode label token IDs into text and print it.  \n",
    "- `show_modality(tokens, modality: str)`: Dispatch tokens to the appropriate display function based on modality.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_im_from_tensor(tensor_or_array):\n",
    "    if isinstance(tensor_or_array, torch.Tensor):\n",
    "        tensor_or_array = tensor_or_array.detach().cpu()\n",
    "        array = tensor_or_array.numpy()\n",
    "    else:\n",
    "        array = tensor_or_array\n",
    "    \n",
    "    array = np.clip(array, 0, 1)\n",
    "    array = array.transpose(1, 2, 0)\n",
    "    \n",
    "    array = (array * 255).astype(np.uint8)\n",
    "\n",
    "    image = Image.fromarray(array)\n",
    "    return image\n",
    "\n",
    "def get_gif_bytes_from_tensor(frames: torch.Tensor, fps: int = 3) -> bytes:\n",
    "\n",
    "    # 1. CPU → numpy and clamp\n",
    "    frames_np = frames.cpu().numpy()\n",
    "    frames_np = np.clip(frames_np, 0, 1)\n",
    "\n",
    "    frames_np = frames_np.transpose(1, 2, 3, 0)\n",
    "    frames_np = (frames_np * 255).astype(np.uint8)\n",
    "    pil_frames = [Image.fromarray(f) for f in frames_np]\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    pil_frames[0].save(\n",
    "        buf,\n",
    "        format='GIF',\n",
    "        save_all=True,\n",
    "        append_images=pil_frames[1:],\n",
    "        duration=int(1000 / fps),\n",
    "        loop=0\n",
    "    )\n",
    "    buf.seek(0)\n",
    "    return buf.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_from_sample(dataset, idx, input_modality):\n",
    "    input_tensor = dataset[idx][input_modality]\n",
    "    n_tokens_input = input_tensor.shape[0]\n",
    "    enc_input_tokens = input_tensor.unsqueeze(0).to(device)\n",
    "    enc_input_positions = torch.arange(n_tokens_input, device=device).unsqueeze(0)\n",
    "    enc_input_modalities = selected_modalities.index(input_modality) * torch.ones(1, n_tokens_input, device=device, dtype=torch.long)\n",
    "    return enc_input_tokens, enc_input_positions, enc_input_modalities\n",
    "\n",
    "def token_ids_to_image(token_ids, image_tokenizer, to_pil=False):\n",
    "    n_tokens = token_ids.numel()\n",
    "    side = int(math.sqrt(n_tokens))\n",
    "    token_ids = token_ids.reshape(1,side,side).to(device)\n",
    "    reconst = image_tokenizer.decode(token_ids)\n",
    "    reconst = (reconst[0].float().cpu())\n",
    "    if to_pil:\n",
    "        reconst = TF.to_pil_image(reconst)\n",
    "    return reconst\n",
    "\n",
    "def tokens_ids_to_audio(token_ids, audio_tokenizer):\n",
    "    num_quantizers = 32\n",
    "    token_ids = token_ids.reshape(1,num_quantizers,-1).to(device)\n",
    "    reconst = audio_tokenizer.decode(token_ids.clamp(0,2047).cpu())\n",
    "    reconst = reconst.squeeze(0)\n",
    "    player = Audio(reconst, rate=24_000)\n",
    "    display(player)\n",
    "    return reconst\n",
    "\n",
    "def tokens_ids_to_gif(token_ids,video_tokenizer):\n",
    "    num_frames = 8\n",
    "    model_bucket_len = 8\n",
    "    nb_div = math.ceil((num_frames + 1) / model_bucket_len)\n",
    "    \n",
    "    n_tokens = token_ids.numel() // nb_div\n",
    "    side = int(math.sqrt(n_tokens))\n",
    "    token_ids = token_ids.reshape(1,nb_div,side,side).to(device)\n",
    "\n",
    "    reconst = video_tokenizer.decode(token_ids)\n",
    "    reconst = reconst[0].float().cpu()\n",
    "    # save_video(reconst, os.path.join(out_path,\"fish.gif\"))\n",
    "    \n",
    "    reconst_gif = get_gif_bytes_from_tensor(reconst)\n",
    "    gif = IPyImage(reconst_gif, format = 'gif')\n",
    "    display(gif)\n",
    "\n",
    "def tokens_ids_to_label(tokens_ids, label_tokenizer):\n",
    "    print(label_tokenizer.decode(tokens_ids.cpu()))\n",
    "\n",
    "def show_modality(tokens, modality: str):\n",
    "    if modality.__contains__('rgb') or modality.__contains__('depth') :\n",
    "        token_ids_to_image(tokens, image_tokenizer, to_pil=True).show()\n",
    "    if modality.__contains__('audio') :\n",
    "        tokens_ids_to_audio(tokens, audio_tokenizer)\n",
    "    if modality.__contains__('video'):\n",
    "        tokens_ids_to_gif(tokens,video_tokenizer)\n",
    "    if modality.__contains__('label'):\n",
    "        tokens_ids_to_label(tokens,label_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate & Display\n",
    "\n",
    "Now that our helper utilities are in place, we can define the core generation function that transforms one modality into another. This function will:\n",
    "\n",
    "1. **Take as input**:  \n",
    "   - `input_mod` (str): source modality (e.g., `\"audio\"`, `\"image\"`, `\"video\"`, `\"label\"`, `\"depth\"`).  \n",
    "   - `target_mod` (str): modality to generate (e.g., `\"video\"`, `\"label\"`, etc.).  \n",
    "   - `nb_iteration` (int): number of samples/iterations to process.  \n",
    "   - `num_steps` (int): number of diffusion or autoregressive steps per sample.  \n",
    "   - `temp` (float): sampling temperature for stochastic decoding.  \n",
    "   - `top_p` (float): nucleus (p-value) sampling threshold.  \n",
    "   - `top_k` (int): top-k sampling cutoff.  \n",
    "\n",
    "2. **Run them through** the Nano4M model in inference mode to generate the output token sequence.  \n",
    "3. **Decode the output tokens** back into the target modality for visualization or playback.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_display(input_mod: str,  target_mod: str, nb_iteration: int = 0,\n",
    "                         num_steps= 1 , temp= 0.7, top_p = 0.0, top_k = 0):\n",
    "    for i in range(nb_iteration):\n",
    "        sample_idx = i\n",
    "\n",
    "        x_tokens, x_positions, x_modalities = construct_input_from_sample(img_dataset2, idx=sample_idx, input_modality=input_mod)\n",
    "        show_modality(x_tokens, input_mod)\n",
    "        \n",
    "        num_steps, temp, top_p, top_k = 1, 0.7, 0.0, 0.0\n",
    "        \n",
    "        pred_tokens, x_tokens, x_positions, x_modalities = model.generate_one_modality_roar(\n",
    "            x_tokens, x_positions, x_modalities, target_mod=target_mod,\n",
    "            num_steps=num_steps, temp=temp, top_p=top_p, top_k=top_k,\n",
    "        )\n",
    "        show_modality(pred_tokens, target_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagaperso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
