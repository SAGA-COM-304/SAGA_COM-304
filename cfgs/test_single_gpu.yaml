# Configuration pour 2 GPUs basée sur multiclevr_d6-6w512

run_name: auto
output_dir: ./outputs/auto

# Global variables
global_vars:
  batch_size: 64  # batch size par GPU (128 global batch size)
  modalities: ["rgb", "depth", "audio", "video"]
  vocab_sizes: [64000, 64000, 64000, 64000]
  max_seq_lens: [256, 256, 256, 256]
  input_alphas: [1.0, 1.0, 1.0, 1.0]  # Activé pour toutes les modalités
  target_alphas: [1.0, 1.0, 1.0, 1.0]  # Activé pour toutes les modalités
  input_tokens_range: [1, 128]
  target_tokens_range: [1, 128]

# Training
batch_size: ${global_vars.batch_size}
total_tokens: 5000  # en millions de tokens
warmup_tokens: 500  # en millions de tokens
num_tokens_per_sample: 256
lr: 0.0006
min_lr: 0.000001
weight_decay: 0.05
clip_grad: 1.0
dtype: fp16

# Eval
eval_freq: 100  # en millions de tokens
save_ckpt_freq: 1000  # en millions de tokens

# Logging
log_wandb: True
wandb_project: COM304_nano4M
wandb_entity: SAGA_COM-304
wandb_run_name: auto

# Model config
model_config:
  _target_: nanofm.models.fourm.FourM
  enc_tokens_read_key: enc_tokens
  dec_tokens_read_key: dec_tokens
  enc_modalities_read_key: enc_modalities
  dec_modalities_read_key: dec_modalities
  enc_positions_read_key: enc_positions
  dec_positions_read_key: dec_positions
  enc_pad_mask_read_key: enc_pad_mask
  dec_pad_mask_read_key: dec_pad_mask
  modalities: ${global_vars.modalities}
  vocab_sizes: ${global_vars.vocab_sizes}
  max_seq_lens: ${global_vars.max_seq_lens}
  dim: 512
  enc_depth: 6
  dec_depth: 6
  head_dim: 64
  per_modality_loss_avg: True

# Train loader config
train_loader_config:
  _target_: nanofm.data.multimodal.create_multimodal_masked_dataloader
  root_dir: /work/com-304/SAGA/tokens_13_05/
  split: train
  modalities: ${global_vars.modalities}
  vocab_sizes: ${global_vars.vocab_sizes}
  max_seq_lens: ${global_vars.max_seq_lens}
  overlap_vocab: True
  overlap_posembs: True
  input_alphas: ${global_vars.input_alphas}
  target_alphas: ${global_vars.target_alphas}
  input_tokens_range: ${global_vars.input_tokens_range}
  target_tokens_range: ${global_vars.target_tokens_range}
  sample_from_k_augmentations: 10
  text_tokenizer_path: gpt2
  text_max_length: 256
  batch_size: ${global_vars.batch_size}
  infinite: True
  num_workers: 10
  pin_memory: True
  shuffle: True
  drop_last: True
  distributed: True

# Eval loader config
eval_loader_config:
  _target_: nanofm.data.multimodal.create_multimodal_masked_dataloader
  root_dir: /work/com-304/SAGA/tokens_13_05/
  split: eval
  modalities: ${global_vars.modalities}
  vocab_sizes: ${global_vars.vocab_sizes}
  max_seq_lens: ${global_vars.max_seq_lens}
  overlap_vocab: True
  overlap_posembs: True
  input_alphas: ${global_vars.input_alphas}
  target_alphas: ${global_vars.target_alphas}
  input_tokens_range: ${global_vars.input_tokens_range}
  target_tokens_range: ${global_vars.target_tokens_range}
  text_tokenizer_path: gpt2
  text_max_length: 256
  batch_size: ${global_vars.batch_size}
  num_workers: 10
  pin_memory: True
  shuffle: False
  drop_last: False
  distributed: True 