
@misc{tian_audiox_2025,
	title = {{AudioX}: {Diffusion} {Transformer} for {Anything}-to-{Audio} {Generation}},
	shorttitle = {{AudioX}},
	url = {http://arxiv.org/abs/2503.10522},
	doi = {10.48550/arXiv.2503.10522},
	abstract = {Audio and music generation have emerged as crucial tasks in many applications, yet existing approaches face significant limitations: they operate in isolation without unified capabilities across modalities, suffer from scarce high-quality, multi-modal training data, and struggle to effectively integrate diverse inputs. In this work, we propose AudioX, a unified Diffusion Transformer model for Anything-to-Audio and Music Generation. Unlike previous domain-specific models, AudioX can generate both general audio and music with high quality, while offering flexible natural language control and seamless processing of various modalities including text, video, image, music, and audio. Its key innovation is a multi-modal masked training strategy that masks inputs across modalities and forces the model to learn from masked inputs, yielding robust and unified cross-modal representations. To address data scarcity, we curate two comprehensive datasets: vggsound-caps with 190K audio captions based on the VGGSound dataset, and V2M-caps with 6 million music captions derived from the V2M dataset. Extensive experiments demonstrate that AudioX not only matches or outperforms state-of-the-art specialized models, but also offers remarkable versatility in handling diverse input modalities and generation tasks within a unified architecture. The code and datasets will be available at https://zeyuet.github.io/AudioX/},
	urldate = {2025-03-21},
	publisher = {arXiv},
	author = {Tian, Zeyue and Jin, Yizhu and Liu, Zhaoyang and Yuan, Ruibin and Tan, Xu and Chen, Qifeng and Xue, Wei and Guo, Yike},
	month = mar,
	year = {2025},
	note = {arXiv:2503.10522 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{noauthor_image-audio_nodate,
	title = {Image-{Audio} dataset},
	url = {https://www.robots.ox.ac.uk/~vgg/data/vggsound/},
}

@misc{pascual_masked_2024,
	title = {Masked {Generative} {Video}-to-{Audio} {Transformers} with {Enhanced} {Synchronicity}},
	url = {http://arxiv.org/abs/2407.10387},
	doi = {10.48550/arXiv.2407.10387},
	abstract = {Video-to-audio (V2A) generation leverages visual-only video features to render plausible sounds that match the scene. Importantly, the generated sound onsets should match the visual actions that are aligned with them, otherwise unnatural synchronization artifacts arise. Recent works have explored the progression of conditioning sound generators on still images and then video features, focusing on quality and semantic matching while ignoring synchronization, or by sacrificing some amount of quality to focus on improving synchronization only. In this work, we propose a V2A generative model, named MaskVAT, that interconnects a full-band high-quality general audio codec with a sequence-to-sequence masked generative model. This combination allows modeling both high audio quality, semantic matching, and temporal synchronicity at the same time. Our results show that, by combining a high-quality codec with the proper pre-trained audio-visual features and a sequence-to-sequence parallel structure, we are able to yield highly synchronized results on one hand, whilst being competitive with the state of the art of non-codec generative audio models. Sample videos and generated audios are available at https://maskvat.github.io .},
	urldate = {2025-03-21},
	publisher = {arXiv},
	author = {Pascual, Santiago and Yeh, Chunghsin and Tsiamas, Ioannis and Serrà, Joan},
	month = jul,
	year = {2024},
	note = {arXiv:2407.10387 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{baade_mae-ast_2022,
	title = {{MAE}-{AST}: {Masked} {Autoencoding} {Audio} {Spectrogram} {Transformer}},
	shorttitle = {{MAE}-{AST}},
	url = {http://arxiv.org/abs/2203.16691},
	doi = {10.48550/arXiv.2203.16691},
	abstract = {In this paper, we propose a simple yet powerful improvement over the recent Self-Supervised Audio Spectrogram Transformer (SSAST) model for speech and audio classification. Specifically, we leverage the insight that the SSAST uses a very high masking ratio (75\%) during pretraining, meaning that the vast majority of self-attention compute is performed on mask tokens. We address this by integrating the encoder-decoder architecture from Masked Autoencoders are Scalable Vision Learners (MAE) into the SSAST, where a deep encoder operates on only unmasked input, and a shallow decoder operates on encoder outputs and mask tokens. We find that MAE-like pretraining can provide a 3x speedup and 2x memory usage reduction over the vanilla SSAST using current audio pretraining strategies with ordinary model and input sizes. When fine-tuning on downstream tasks, which only uses the encoder, we find that our approach outperforms the SSAST on a variety of downstream tasks. We further conduct comprehensive evaluations into different strategies of pretraining and explore differences in MAE-style pretraining between the visual and audio domains.},
	urldate = {2025-03-21},
	publisher = {arXiv},
	author = {Baade, Alan and Peng, Puyuan and Harwath, David},
	month = mar,
	year = {2022},
	note = {arXiv:2203.16691 [eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{ziv_masked_2024,
	title = {Masked {Audio} {Generation} using a {Single} {Non}-{Autoregressive} {Transformer}},
	url = {http://arxiv.org/abs/2401.04577},
	doi = {10.48550/arXiv.2401.04577},
	abstract = {We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensive empirical evaluation, considering both objective metrics and human studies. The proposed approach is comparable to the evaluated baselines, while being significantly faster (x7 faster than the autoregressive baseline). Through ablation studies and analysis, we shed light on the importance of each of the components comprising MAGNeT, together with pointing to the trade-offs between autoregressive and non-autoregressive modeling, considering latency, throughput, and generation quality. Samples are available on our demo page https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT.},
	urldate = {2025-03-21},
	publisher = {arXiv},
	author = {Ziv, Alon and Gat, Itai and Lan, Gael Le and Remez, Tal and Kreuk, Felix and Défossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi},
	month = mar,
	year = {2024},
	note = {arXiv:2401.04577 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{gupta_maskvit_2022,
	title = {{MaskViT}: {Masked} {Visual} {Pre}-{Training} for {Video} {Prediction}},
	shorttitle = {{MaskViT}},
	url = {http://arxiv.org/abs/2206.11894},
	doi = {10.48550/arXiv.2206.11894},
	abstract = {The ability to predict future visual observations conditioned on past observations and motor commands can enable embodied agents to plan solutions to a variety of tasks in complex environments. This work shows that we can create good video prediction models by pre-training transformers via masked visual modeling. Our approach, named MaskViT, is based on two simple design decisions. First, for memory and training efficiency, we use two types of window attention: spatial and spatiotemporal. Second, during training, we mask a variable percentage of tokens instead of a fixed mask ratio. For inference, MaskViT generates all tokens via iterative refinement where we incrementally decrease the masking ratio following a mask scheduling function. On several datasets we demonstrate that MaskViT outperforms prior works in video prediction, is parameter efficient, and can generate high-resolution videos (256x256). Further, we demonstrate the benefits of inference speedup (up to 512x) due to iterative decoding by using MaskViT for planning on a real robot. Our work suggests that we can endow embodied agents with powerful predictive models by leveraging the general framework of masked visual modeling with minimal domain knowledge.},
	urldate = {2025-03-20},
	publisher = {arXiv},
	author = {Gupta, Agrim and Tian, Stephen and Zhang, Yunzhi and Wu, Jiajun and Martín-Martín, Roberto and Fei-Fei, Li},
	month = aug,
	year = {2022},
	note = {arXiv:2206.11894 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{yu_magvit_2023,
	title = {{MAGVIT}: {Masked} {Generative} {Video} {Transformer}},
	shorttitle = {{MAGVIT}},
	url = {http://arxiv.org/abs/2212.05199},
	doi = {10.48550/arXiv.2212.05199},
	abstract = {We introduce the MAsked Generative VIdeo Transformer, MAGVIT, to tackle various video synthesis tasks with a single model. We introduce a 3D tokenizer to quantize a video into spatial-temporal visual tokens and propose an embedding method for masked video token modeling to facilitate multi-task learning. We conduct extensive experiments to demonstrate the quality, efficiency, and flexibility of MAGVIT. Our experiments show that (i) MAGVIT performs favorably against state-of-the-art approaches and establishes the best-published FVD on three video generation benchmarks, including the challenging Kinetics-600. (ii) MAGVIT outperforms existing methods in inference time by two orders of magnitude against diffusion models and by 60x against autoregressive models. (iii) A single MAGVIT model supports ten diverse generation tasks and generalizes across videos from different visual domains. The source code and trained models will be released to the public at https://magvit.cs.cmu.edu.},
	urldate = {2025-03-20},
	publisher = {arXiv},
	author = {Yu, Lijun and Cheng, Yong and Sohn, Kihyuk and Lezama, José and Zhang, Han and Chang, Huiwen and Hauptmann, Alexander G. and Yang, Ming-Hsuan and Hao, Yuan and Essa, Irfan and Jiang, Lu},
	month = apr,
	year = {2023},
	note = {arXiv:2212.05199 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{mizrahi_4m_2023,
	title = {{4M}: {Massively} {Multimodal} {Masked} {Modeling}},
	shorttitle = {{4M}},
	url = {http://arxiv.org/abs/2312.06647},
	doi = {10.48550/arXiv.2312.06647},
	abstract = {Current machine learning models for vision are often highly specialized and limited to a single modality and task. In contrast, recent large language models exhibit a wide range of capabilities, hinting at a possibility for similarly versatile models in computer vision. In this paper, we take a step in this direction and propose a multimodal training scheme called 4M. It consists of training a single unified Transformer encoder-decoder using a masked modeling objective across a wide range of input/output modalities - including text, images, geometric, and semantic modalities, as well as neural network feature maps. 4M achieves scalability by unifying the representation space of all modalities through mapping them into discrete tokens and performing multimodal masked modeling on a small randomized subset of tokens. 4M leads to models that exhibit several key capabilities: (1) they can perform a diverse set of vision tasks out of the box, (2) they excel when fine-tuned for unseen downstream tasks or new input modalities, and (3) they can function as a generative model that can be conditioned on arbitrary modalities, enabling a wide variety of expressive multimodal editing capabilities with remarkable flexibility. Through experimental analyses, we demonstrate the potential of 4M for training versatile and scalable foundation models for vision tasks, setting the stage for further exploration in multimodal learning for vision and other domains.},
	urldate = {2025-03-20},
	publisher = {arXiv},
	author = {Mizrahi, David and Bachmann, Roman and Kar, Oğuzhan Fatih and Yeo, Teresa and Gao, Mingfei and Dehghan, Afshin and Zamir, Amir},
	month = dec,
	year = {2023},
	note = {arXiv:2312.06647 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}
