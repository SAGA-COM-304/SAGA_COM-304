\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment


\begin{document}
\title{Progress report - SAGA}

\author{
  Sacha Godey (362191), Alexis Carreras (361573), Gabriel Taieb (360560), Adrien Bousquié (361516)\\
  \textit{COM-304 Project Progress Report}
}

\maketitle


% ---------- Milestone Progress ----------
\section{Milestone Progress}
We studied different modeling strategies, including autoregressive models, masked models, and multimodal masking. This helped us understand when each approach is best suited. For example, autoregressive models perform well in sequential generation, while masked models are more parallelizable and flexible for reconstruction tasks. We also gained a solid understanding of transformer internals (tokenization, embeddings, attention, ...) and how these components adapt across modalities. A key insight was the ability to reuse the same architecture for different data types (e.g., text and MNIST), reinforcing the feasibility of extending Nano4M to audio and video through appropriate tokenizers.


% ---------- Discussion & Next Steps ----------
\section{Feasibility of Original Plan}
\label{sec:feasibility}
\subsection{Initial Idea and Motivation}
% Expliquer brièvement l’idée de départ du projet et ce qui vous a poussés à la choisir.
Our initial idea was to enhance the 4M model by introducing dynamic modalities such as audio and video. We were motivated by the growing importance of multimodal understanding in real-world applications like video captioning, content generation, and accessibility tools. By focusing on temporal modalities, the aim was to generate four new images at fixed time interval from a starter image and the corresponding audio.



\subsection{Updated Problem Statement}
% Reformuler clairement la problématique que vous ciblez maintenant, en tenant compte des éléments ci-dessus.
Initially, we planned to start by designing and training our own audio and video tokenizers from scratch. However, based on the complexity of the task and the need to validate other critical parts of the pipeline (such as training, modality integration, and inference optimization), we decided to adjust our strategy.
We will now begin with pretrained tokenizers to ensure the rest of the system functions as expected. Once we gain deeper insights into training dynamics and suitable inference hyperparameters, we will move to a second milestone where we develop and fine-tune our own tokenizers.
This revised approach allows us to de-risk the project while still aiming for the same final goals. For instance, we will be experiencing soon to see how effective existing pre-trained tokenizers are and to what extent they can be reused or compressed for our use case.


\section{Next Steps}
\subsection{Preparing the data}
To adapt our model for temporal analysis, we require a new dataset. We have chosen to use the VGGSound Dataset \cite{chen_vggsound_2020}. This dataset is well-suited to our needs, as it contains labeled video with a corresponding audio track. 
The videos in VGGSound are simple and do not depict complex scenes. This aligns well with our current objective, develop a model that demonstrates the potential of incorporating temporality into 4M.
Before feeding the data into our model, it must be preprocessed and formatted to match our model’s structure.
The first step will be to filter the dataset to extract videos with clear and simple audio-visual content, ideally, calm natural scenes with animals. 
Both video and audio components will need to conform to specific encoding formats:

- \textbf{Video}: Each video will be resized to a resolution of 256×256 pixels. From each selected video, we will extract 5 frames at a given time intervals. The first frame will represent the input (i.e., the "current" image), while the following four frames will represent the subsequent five seconds, serving as the model's temporal targets.

- \textbf{Audio:} Based on the time interval between the first and last sampled video frames, we will extract the corresponding audio segment from each video. To ensure consistency across the dataset, all audio clips will be resampled to a unified sampling rate between 22 kHz and 24 kH which provides a good trade-off between capturing ambient sound detail and keeping the data lightweight. Additionally, each audio track will be converted to mono to reduce complexity and standardize the input to a single channel, making it more efficient to process within the model.

\subsection{Multimodal Integration}
Starting by using these pretrained tokenizers for audio and video (e.g., Mimi tokenizer from the Moshi model \cite{defossez_moshi_2024} for spectrograms, Cosmos tokenizer \cite{nvidia_cosmos_2025} for video) to quickly obtain token sequences for our new modalities, the tokens will be directly fed into the Nano4M architecture without modifying it initially, allowing us to evaluate how well the model handles audio and video inputs as temporal modalities.
% 
Our goal is first to obtain a working baseline, then compare different tokenizers to identify which are best suited for our tasks. This will help us choose the most efficient and meaningful token representations for audio and video generation.

We will focus on lightweight tasks compatible with our computational constraints:
\begin{itemize}
    \item \textbf{Ambient audio generation:} Given an image (e.g., jungle), generate a fitting ambient sound.
    \item \textbf{Next-frame prediction:} Predict the next 5 frames from a single image, sampled at fixed time intervals to avoid full video generation and reduce token length.
\end{itemize}

These tasks will help us validate the modality integration and guide further improvements to the architecture and tokenizer design.


\subsection{Audio Tokenizer Strategy}

We plan to build an unsupervised audio tokenizer based on vector quantization (VQ), using an encoder-decoder structure. The encoder transforms audio inputs into latent vectors, which are quantized via a codebook (e.g., k-means), then decoded to reconstruct the original input. These discrete tokens will serve as input to Nano4M.

We will explore two input formats: raw waveform and spectrograms. Spectrograms provide a perceptually aligned time-frequency representation, while raw audio retains more detail but is harder to model.

Alternatively, we may leverage pretrained audio tokenizers trained on large datasets, using their outputs as guidance to adapt our own. (i.e Mimi architecture \cite{defossez_moshi_2024}) This can help avoid overfitting to tasks like speech recognition and focus on our goals, such as ambient sound generation and cross-modal alignment.


\subsection{Video Tokenizer Strategy}
% Dire comment vous comptez construire ou adapter le tokenizer vidéo (patching spatio-temporel, MAGVIT...)

We plan to initially treat a
\emph{video clip as a flat sequence of frame tokens} produced by our existing RGB
tokenizer.  This lets us validate the Nano4M backbone on temporal data before we
invest GPU time in training a dedicated video tokenizer.

\begin{enumerate}
    \item \textbf{Next‑4‑frame prediction \& any-to-any generation}\\
          We train the current Nano4M Transformer to reconstruct the \emph{masked} tokens of the last four frames.
    \item \textbf{Temporal super‑resolution.}\\
          Using span masking along the time axis, the model hallucinates every other frame (2 fps $\rightarrow$ 4 fps).  Evaluation via Fréchet Video Distance (FVD) \cite{unterthiner_towards_2019}.
    \item \textbf{True video tokenizer.}\\
          We make our own video tokenizer inspired of \emph{VidTok} \cite{tang_vidtok_2024} tokenizer.
    \item \textbf{Integration \& any‑to‑any generation.}\\
          The codes are registered in the tokenizer registry; Nano4M can now do chains such as \emph{caption + first frame $\rightarrow$ 4 next frames}.
          We will report FVD 2048 and provide qualitative demos on the project website.
\end{enumerate}

If 3 proves too compute‑heavy, we will still submit 1–2 and an
ablation study comparing frame‑wise vs. clip‑wise token streams.



\subsection{Week-by-Week Plan}
% Proposer une répartition des tâches sur les semaines à venir (ex: semaine 11 = tokenizer audio, etc.)
 In \textbf{Week 1}, we will focus on data augmentation and the implementation of pretrained tokenizers for both audio and video, without modifying the architecture. The objective is to ensure the new modalities are functional i.e., producing properly formatted outputs and to begin training. \textbf{Week 2} will be dedicated to optimizing the architecture and identifying suitable hyperparameters to achieve satisfactory results. In \textbf{Week 3}, we will begin developing our own tokenizers from scratch for both modalities subject to computational limitations. Finally, \textbf{Week 4} will be reserved for final improvements and the writing of the report. To streamline the workflow, we plan to split into two teams: \textit{Sacha and Alexis} will focus on the audio modality, while \textit{Gabriel and Adrien} will handle the video modality.


% ---------- Author Contribution Statement ----------
\section{Author Contribution Statement}
A.B maintained the basic collaborating tools such as the Zotero, Overleaf and GitHub used for our project.


S.G found an appropriate dataset and A.B designed the plan to use the dataset for our model.


S.G and A.C designed the audio tokenizer strategy, the week by week plan, and thought about the multimodal integration plan for the project proposal.


G.T and A.B designed the video tokenizer strategy for the project proposal.


% ---------- Bibliography ----------
\bibliographystyle{IEEEtran}
\bibliography{references.bib}

\end{document}
